# Notes for Facilitators

## Pre-Session Setup
- Screen-record the entire screen — **ask for consent first**
- Record the participant — **ask for consent first**
- Clear browsing history, cookies, data, and **log out of the Chrome extension**
- *Tip: create a fresh Chrome profile to avoid autofill or cached states*

---

# Facilitator Script (Read at the Start)

### 1. Welcome + Consent (1 min)
“Thanks for helping! I’ll ask you to perform a few tasks on the app we created.  
There are no wrong answers — we’re testing the **app’s design**, not you.  
Please think out loud, describe what you expect, and what confuses you.  
I’ll stay quiet unless you get completely stuck.”

### 2. Think-Aloud Reminder (15 sec)
“Please say what you're thinking — what you expect to happen, what you're looking for, and what confuses you.”

### 3. Intervention Rule
“I won’t help unless you’re truly stuck. Feel free to try things, undo things, or explore.”

### 4. Begin Tasks
“I’ll give you tasks one at a time. Let’s start!”

---

# During the Session: What to Observe

## General Observations
- Where they hesitate or scan the screen
- What they say they expect before clicking
- Facial expressions, sighs, repeated taps/clicks

## Task-Specific Observations
- **Task 1:** Understanding of field-of-interest selection
- **Task 2:** Notice reflection questions? Metadata clear?
- **Task 3:** Recognize extension UI? Faster/easier?
- **Task 4:** Expect autosave? Notice delete confirmation?
- **Task 5:** Understand swiping? Notice progress indicators?
- **Task 6:** Understand approval % and total votes?
- **Task 7:** Can they find the Stats page? Overwhelming or meaningful?

---

# Task List

Below is the task list in table format.

| **Task Title** | **Instruction (what we say)** | **Rationale (why this matters)** |
|----------------|--------------------------------|-----------------------------------|
| **1. Create an Account** | “Please create a new account and set up your profile, including selecting your fields of interest.” | Tests onboarding clarity, form layout, and whether users understand that fields of interest personalize later parts of the app. Also checks the visibility of validation errors and whether the signup flow feels intuitive. |
| **2. Add an Item to PauseCart (Manual Link Flow)** | “Imagine you found two items online you’re tempted to buy. Add a new item to your PauseCart using its Amazon link and fill out the reflection questions.” | Tests the core PauseCart experience: copy/paste workflow, metadata loading expectations, and whether the reflection questions feel clear, discoverable, and meaningful. Evaluates the gulf of execution (what do I click?) and gulf of evaluation (did something happen after I added it?).|
| **3. Add an Item Using the Chrome Extension** | “Open an Amazon product page and use the floating Chrome extension button to add two items.” | Tests cross-surface interaction: whether users recognize the floating button, whether the scraping metadata is trustworthy/understandable, and whether the extension workflow feels faster or more natural. Helps identify mismatches between app vs extension mental models.|
| **4. Edit & Manage Items + View AI Insight** | “Go to your PauseCart, open an item, edit a reflection answer, delete another item, and view the AI Insight.” | Tests item detail discoverability, update flows, and confirmation dialogs. Reveals confusion about editing vs deleting vs marking purchased. Ensures the information hierarchy in the PauseCart is intuitive. |
| **5. Complete a Daily SwipeSense Queue** | “Please complete your daily swipe queue by approving or rejecting each item from other users’ wishlists.” | Tests the most unique interaction in the system: how users understand swiping mechanics, whether they notice progress indicators, how natural the gestures/buttons feel, and whether queue order/pacing is clear. Also reveals confusion about who added the items and why swiping is required. |
| **6. View Community Feedback on Your Items** | “Check your PauseCart again and find the community feedback for your items.” | Tests whether users realize feedback unlocks only after queue completion, whether they understand the meaning of aggregated stats, and whether the feedback display is interpretable (no shaming, clear phrasing). Also tests visibility: can users find the feedback panel? |
| **7. Mark an Item as Purchased + Explore Stats Page** | “Mark one item as purchased and then check how the Stats page updates.” | Tests the end-to-end loop from decision → action → insight. Reveals whether users understand the bookkeeping fields, whether the stats page communicates meaningful information, and whether navigation between views is obvious. Also validates whether users notice the AI insight panel.|
| **8. Log in as Alice (seed user)** | “Log in as Alice (alice@test.com / test123). Complete her queue and view her stats and community feedback.” | Tests behavior with pre-populated account and understanding of community stats. |

---

# Debrief Script (10–15 min)

## General Questions
- “How did the experience feel overall?”
- “Which part felt most intuitive?”
- “Which part felt confusing or unexpected?”

## Moment-Specific Questions
- “I noticed you hesitated when ___ — what were you thinking?”
- “If you could redesign one part of the app, what would you change?”
- “Did anything feel missing in helping you make decisions?”

## Preferences
- “Did PauseCart feel helpful or annoying?”
- “Was the swiping feature intuitive? Why or why not?”
- “Did the Stats dashboard give you useful information?”
